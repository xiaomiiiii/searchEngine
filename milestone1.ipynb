{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder 0\n",
      "Processing folder 1\n",
      "Processing folder 2\n",
      "Processing folder 3\n",
      "Processing folder 4\n",
      "Processing folder 5\n",
      "Processing folder 6\n",
      "Processing folder 7\n",
      "Processing folder 8\n",
      "Processing folder 9\n",
      "Processing folder 10\n",
      "Processing folder 11\n",
      "Processing folder 12\n",
      "Processing folder 13\n",
      "Processing folder 14\n",
      "Processing folder 15\n",
      "Processing folder 16\n",
      "Processing folder 17\n",
      "Processing folder 18\n",
      "Processing folder 19\n",
      "Processing folder 20\n",
      "Processing folder 21\n",
      "Processing folder 22\n",
      "Processing folder 23\n",
      "Processing folder 24\n",
      "Processing folder 25\n",
      "Processing folder 26\n",
      "Processing folder 27\n",
      "Processing folder 28\n",
      "Processing folder 29\n",
      "Processing folder 30\n",
      "Processing folder 31\n",
      "Processing folder 32\n",
      "Processing folder 33\n",
      "Processing folder 34\n",
      "Processing folder 35\n",
      "Processing folder 36\n",
      "Processing folder 37\n",
      "Processing folder 38\n",
      "Processing folder 39\n",
      "Processing folder 40\n",
      "Processing folder 41\n",
      "Processing folder 42\n",
      "Processing folder 43\n",
      "Processing folder 44\n",
      "Processing folder 45\n",
      "Processing folder 46\n",
      "Processing folder 47\n",
      "Processing folder 48\n",
      "Processing folder 49\n",
      "Processing folder 50\n",
      "Processing folder 51\n",
      "Processing folder 52\n",
      "Processing folder 53\n",
      "Processing folder 54\n",
      "Processing folder 55\n",
      "Processing folder 56\n",
      "Processing folder 57\n",
      "Processing folder 58\n",
      "Processing folder 59\n",
      "Processing folder 60\n",
      "Processing folder 61\n",
      "Processing folder 62\n",
      "Processing folder 63\n",
      "Processing folder 64\n",
      "Processing folder 65\n",
      "Processing folder 66\n",
      "Processing folder 67\n",
      "Processing folder 68\n",
      "Processing folder 69\n",
      "Processing folder 70\n",
      "Processing folder 71\n",
      "Processing folder 72\n",
      "Processing folder 73\n",
      "Processing folder 74\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from lxml.html.clean import Cleaner\n",
    "import re\n",
    "import collections\n",
    "\n",
    "def get_json(file):\n",
    "    with open(file) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def inverted_index():\n",
    "    index = {}\n",
    "    for i in range(75):\n",
    "        print \"Processing folder \" + str(i)\n",
    "        if i < 74:\n",
    "            doc_num = 500\n",
    "        else:\n",
    "            doc_num = 497\n",
    "        for j in range(doc_num):\n",
    "            doc_id = \"%s\"%i + \"/\" + \"%s\"%j\n",
    "            doc_path = \"WEBPAGES_RAW/\" + doc_id\n",
    "            #path in WEBPAGES_RAW\n",
    "            with open(doc_path) as page:\n",
    "                raw_content = page.read()\n",
    "                #remove css, scripts, js in the HTML file\n",
    "                cleaner = Cleaner(style = True, scripts = True, comments = True, javascript = True, page_structure = False, safe_attrs_only = False)\n",
    "                if raw_content:\n",
    "                    #TODO: 处理04/288报错document is empty，暂时用了try except。04/288里全是乱码。\n",
    "                    try:\n",
    "                        content= cleaner.clean_html(raw_content)\n",
    "                    except:\n",
    "                        continue\n",
    "                    #remove tags. only leave texts\n",
    "                    reg = re.compile('<[^>]*>')\n",
    "                    content = reg.sub(' ', content)\n",
    "                    if content:\n",
    "                        #TODO：只做了移除符号，大小写转换，没加stemming，stopWord等处理\n",
    "                        content = re.sub(r\"[^a-zA-Z0-9]\", \" \", content.lower())\n",
    "                        for term in content.split():\n",
    "                            #control the length of the term\n",
    "                            if len(term) < 3 or len(term) > 20:\n",
    "                                continue\n",
    "                            if term not in index:\n",
    "                                index[term] = {}\n",
    "                            if doc_id not in index[term]:\n",
    "                                index[term][doc_id] = {\n",
    "                                    \"tf\": 0,\n",
    "                                    \"tf-idf\": 0,\n",
    "                                    \"other_info\": \"to be improved in milestone 2\"\n",
    "                                }\n",
    "                            index[term][doc_id][\"tf\"] += 1\n",
    "    with open(\"index.json\",\"w\") as f:\n",
    "        json.dump(index, f)\n",
    "    return index\n",
    "\n",
    "#TODO：增加index内容，排序，relevance scoring function，减小index（现在是570M），减少搜索时间，词组搜索，GUI。。。\n",
    "\n",
    "\n",
    "def search(user_input):\n",
    "    #TODO: 对user input的处理\n",
    "    user_input = user_input.lower()\n",
    "    urls=[]\n",
    "    book_keeping = get_json(\"WEBPAGES_RAW/bookkeeping.json\")\n",
    "    index = get_json(\"index.json\")\n",
    "    #index = get_json(\"test.json\")\n",
    "    if user_input in index:\n",
    "        for doc in index[user_input]:\n",
    "            urls.append(book_keeping[doc])\n",
    "            if len(urls) >= 10:\n",
    "                break\n",
    "    return urls\n",
    "\n",
    "# my_index = inverted_index()\n",
    "# print \"Finished\"\n",
    "user_input = raw_input(\"Please input your search keyword: \")\n",
    "search_result = search(user_input)\n",
    "if search_result:\n",
    "    for url in search_result:\n",
    "        print url\n",
    "else:\n",
    "    print \"No related content.\"\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
